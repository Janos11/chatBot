# Open WebUI + Ollama

This containerized setup provides a web interface to interact with locally hosted Ollama models (like TinyLlama, Phi, Mistral, etc.).

## ðŸ”§ Setup

### 1. Prerequisites

- Docker & Docker Compose installed
- Ollama running on the host (accessible via `http://host.docker.internal:11434` or your Piâ€™s IP)

### 2. Clone and Launch

```bash
git clone https://your-repo-or-local-path/open-webui
cd open-webui
docker compose up -d
```


